{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46641326-1e71-4d7a-a13b-e3584ab5cb1f",
   "metadata": {
    "id": "46641326-1e71-4d7a-a13b-e3584ab5cb1f"
   },
   "source": [
    "# Model fine-tuning\n",
    "### XLM-RoBERTa and mDeBERTa models fine-tuning on MultiCW and OOD datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7fd0d-9d7e-43be-9257-644125af64e9",
   "metadata": {
    "id": "3eb7fd0d-9d7e-43be-9257-644125af64e9"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9164a8d-2e16-45c1-b052-79ff7da7e8cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Setup project paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b1d6c59d2b5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join, exists\n",
    "from py_markdown_table.markdown_table import markdown_table\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# ANSI Highlighting: https://stackoverflow.com/a/21786287\n",
    "h_red = '\\x1b[1;30;41m'\n",
    "h_green = '\\x1b[1;30;42m'\n",
    "h_yellow = '\\x1b[1;30;43m'\n",
    "h_stop = '\\x1b[0m'\n",
    "\n",
    "## Setup project paths:\n",
    "project_path = os.getcwd()\n",
    "models_path = join(project_path, \"Models\")\n",
    "\n",
    "datasets_path = join(project_path, \"Source datasets\")\n",
    "multicw_path = join(project_path, 'Final-dataset')\n",
    "multiclaim_path = join(datasets_path, \"MultiClaim\")\n",
    "lesa_dst_dir = join(datasets_path, 'LESA-EACL-2021')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb31e5eb4ab1684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets\n",
    "Loading the MultiCW and OOD datasets for the purpose of models fine-tuning and their evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585cff8b46c36f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MultiCW:\n",
      "Train set: 86691\n",
      "Dev set: 18491\n",
      "Test set: 18540\n",
      "Out-of-dist set: 29647\n"
     ]
    }
   ],
   "source": [
    "# Load MultiCW model\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "languages = pd.read_csv(join('Final-dataset', 'multicw-full.csv'))['lang'].unique()\n",
    "\n",
    "multicw_path = join(\"Final-dataset\")\n",
    "multicw_train = pd.read_csv(join(multicw_path, \"multicw-train.csv\")).astype({'label':'int'})\n",
    "multicw_dev = pd.read_csv(join(multicw_path, \"multicw-dev.csv\")).astype({'label':'int'})\n",
    "multicw_test = pd.read_csv(join(multicw_path, \"multicw-test.csv\")).astype({'label':'int'})\n",
    "multicw_ood = pd.read_csv(join(multicw_path, \"multicw-ood.csv\")).astype({'label':'int'})\n",
    "multicw_ood['style'] = multicw_ood['style'].replace('structured', 'struc')\n",
    "multicw_ood['text'] = multicw_ood['text'].fillna(\"\").astype(str)\n",
    "\n",
    "print(f'Loaded MultiCW:')\n",
    "print(f'Train set: {multicw_train.shape[0]}')\n",
    "print(f'Dev set: {multicw_dev.shape[0]}')\n",
    "print(f'Test set: {multicw_test.shape[0]}')\n",
    "print(f'Out-of-dist set: {multicw_ood.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07373bf5-a7a4-4a8f-a93a-d1054d4b07cf",
   "metadata": {
    "id": "07373bf5-a7a4-4a8f-a93a-d1054d4b07cf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Models\n",
    "Implementation of the used models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120cb5dc7290406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1nnytAhaNAny",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nnytAhaNAny",
    "outputId": "8cddf7b0-303c-4de4-d926-24ecd5edbd25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "\n",
    "class XLMRobertaModel():\n",
    "    \"\"\" Model finetuning and inference on the LESA dataset using XLMRoberta \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_len = 256\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.final_model = None\n",
    "\n",
    "        self.preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "            'xlm_roberta_base_multi',\n",
    "            sequence_length=self.max_len\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_name) -> bool:\n",
    "        try:\n",
    "            # 1. Instantiate the model with the known preset (architecture)\n",
    "            self.final_model = keras_hub.models.XLMRobertaTextClassifier.from_preset(\n",
    "                'xlm_roberta_base_multi',\n",
    "                num_classes=2,\n",
    "                preprocessor=self.preprocessor,\n",
    "                dropout=0.2\n",
    "            )\n",
    "    \n",
    "            # 2. Load the weights manually from the path\n",
    "            weights_path = os.path.join('Models', f'{model_name}.weights.h5')\n",
    "            self.final_model.load_weights(weights_path)\n",
    "    \n",
    "            print('Model loaded successfully.')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_claims(self, test_set: DataFrame, verbose=False) -> tuple:\n",
    "        \"\"\"Performs inference on the testing data and evaluates results.\"\"\"\n",
    "\n",
    "        print('Running classification:')\n",
    "        metrics = self.final_model.predict(x=test_set['text'].to_numpy(), batch_size=self.batch_size)\n",
    "\n",
    "        # Testing\n",
    "        results = np.argmax(metrics, axis=1)\n",
    "        print('Done.')\n",
    "        # Print sentences with classifications\n",
    "        if verbose:\n",
    "            for i, text in enumerate(test_set['text']):\n",
    "                print(\"LESA classification: '{text}' {c} a claim.\".\n",
    "                      format(text=text, c=gh_start + \"is\" if results[i] == 1 else rh_start + \"is not\") + h_stop)\n",
    "\n",
    "        # Compare against ground-truth\n",
    "        ground_truth = test_set['label'].to_numpy()\n",
    "        report = classification_report(ground_truth, results, output_dict=True)\n",
    "        report_str = str(classification_report(ground_truth, results))\n",
    "\n",
    "        return report, report_str\n",
    "\n",
    "    def train_model(self, train_set: DataFrame, dev_set: DataFrame, epochs=1, learn_rate=3e-5, model_name='', lang='en'):\n",
    "        \"\"\"\n",
    "        Train the XLMRoberta model with the given parameters.\n",
    "\n",
    "        :param learn_rate: Learning rate.\n",
    "        :param train_set: Training dataset.\n",
    "        :param dev_set: Validation dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param model_name: Model will be saved to the directory named by this value.If left blank, the model won't save.\n",
    "        :param lang: Training dataset language(s). Needed for naming conventions.\n",
    "        :return: Trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        model_name_path = f\"{model_name}-{lang}-{epochs}e\"\n",
    "        path = os.path.join('Models', 'LESA', 'models', model_name_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "        self.final_model = keras_hub.models.XLMRobertaTextClassifier.from_preset(\n",
    "            'xlm_roberta_base_multi',\n",
    "            num_classes=len(train_set['label'].unique()),\n",
    "            preprocessor=self.preprocessor,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        for layer in self.final_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        self.final_model.compile(\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=keras.optimizers.Adam(learn_rate),\n",
    "            jit_compile=True,\n",
    "        )\n",
    "\n",
    "        self.final_model.fit(x=train_set['text'].to_numpy(), y=train_set['label'].to_numpy(), batch_size=self.batch_size,\n",
    "                             epochs=epochs, validation_data=(dev_set['text'].to_numpy(), dev_set['label'].to_numpy()))\n",
    "\n",
    "        model_save_path = os.path.join(path, 'model.keras')\n",
    "        self.final_model.save(model_save_path)\n",
    "\n",
    "    def inference(self, texts: list[str]) -> list[bool]:\n",
    "        \"\"\"Returns a list of booleans: True if classified as a claim (class 1), else False.\"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise ValueError(\"Input must be a list of strings.\")\n",
    "        if not all(isinstance(t, str) for t in texts):\n",
    "            raise ValueError(\"All items in input list must be strings.\")\n",
    "    \n",
    "        predictions = self.final_model.predict(texts)  # Must be list[str]\n",
    "        classifications = np.argmax(predictions, axis=1)  # shape (batch_size,)\n",
    "        \n",
    "        return (classifications == 1).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb81e0d1219ede9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### mDeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "o-kYQPkJyLcm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-kYQPkJyLcm",
    "outputId": "2676b48b-f4ec-415d-e5af-4d60d6d48d01"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "\n",
    "class MDeBertaModel():\n",
    "    \"\"\"  \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=''):\n",
    "        self.max_len = 256\n",
    "        self.batch_size = 32\n",
    "        self.final_model = None\n",
    "        \n",
    "        self.preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "            'deberta_v3_base_multi',\n",
    "            sequence_length=self.max_len\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_name) -> bool:\n",
    "        try:\n",
    "            model_path = os.path.join('Models', model_name + '.keras')\n",
    "    \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Saved model not found at: {model_path}\")\n",
    "    \n",
    "            print(f\"Loading full model from: {model_path}\")\n",
    "            self.final_model = keras.models.load_model(model_path)\n",
    "            print(\"Model loaded successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def detect_claims(self, test_set: DataFrame, verbose=False) -> tuple:\n",
    "        \"\"\"Performs inference on the testing data and evaluates results.\"\"\"\n",
    "\n",
    "        print('Running classification:')\n",
    "        metrics = self.final_model.predict(x=test_set['text'].to_numpy(), batch_size=self.batch_size)\n",
    "\n",
    "        # Testing\n",
    "        results = np.argmax(metrics, axis=1)\n",
    "        print('Done.')\n",
    "        # Print sentences with classifications\n",
    "        if verbose:\n",
    "            for i, text in enumerate(test_set['text']):\n",
    "                print(\"LESA classification: '{text}' {c} a claim.\".\n",
    "                      format(text=text, c=gh_start + \"is\" if results[i] == 1 else rh_start + \"is not\") + h_stop)\n",
    "\n",
    "        # Compare against ground-truth\n",
    "        ground_truth = test_set['label'].to_numpy()\n",
    "        report = classification_report(ground_truth, results, output_dict=True)\n",
    "        report_str = str(classification_report(ground_truth, results))\n",
    "\n",
    "        return report, report_str\n",
    "\n",
    "    def train_model(self, train_set: DataFrame, dev_set: DataFrame, epochs=1, learn_rate=3e-5, model_name='', lang='en', final_learn_rate_fraction=0.5):\n",
    "        \"\"\"\n",
    "        Train the mDeBerta model with the given parameters.\n",
    "        :param learn_rate: Learning rate.\n",
    "        :param train_set: Training dataset.\n",
    "        :param dev_set: Validation dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param model_name: Model will be saved to the directory named by this value.If left blank, the model won't save.\n",
    "        :param lang: Training dataset language(s). Needed for naming conventions.\n",
    "        :return: Trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.final_model = keras_hub.models.DebertaV3Classifier.from_preset(\n",
    "            'deberta_v3_base_multi',\n",
    "            num_classes=len(set(train_set['label'].unique())),\n",
    "            preprocessor=self.preprocessor,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        for layer in self.final_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        initial_learning_rate = learn_rate\n",
    "        decay_steps = int(len(train_set) // self.batch_size * epochs)   # Number of steps over which the decay is applied\n",
    "        alpha = final_learn_rate_fraction  # Minimum learning rate as a fraction of initial_learning_rate\n",
    "\n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            decay_steps=decay_steps,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        self.final_model.compile(\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            jit_compile=True\n",
    "        )\n",
    "\n",
    "        self.final_model.fit(x=train_set['text'].to_numpy(), y=train_set['label'].to_numpy(), batch_size=self.batch_size,\n",
    "                             epochs=epochs, validation_data=(dev_set['text'].to_numpy(), dev_set['label'].to_numpy()))\n",
    "\n",
    "        model_save_path = os.path.join(path, f\"{model_name}-{lang}-{epochs}e.keras\")\n",
    "        self.final_model.save(model_save_path)\n",
    "        \n",
    "    def inference(self, texts: list[str]) -> list[bool]:\n",
    "        \"\"\"Returns a list of booleans: True if classified as a claim (class 1), else False.\"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise ValueError(\"Input must be a list of strings.\")\n",
    "        if not all(isinstance(t, str) for t in texts):\n",
    "            raise ValueError(\"All items in input list must be strings.\")\n",
    "    \n",
    "        predictions = self.final_model.predict(texts)  # Must be list[str]\n",
    "        classifications = np.argmax(predictions, axis=1)  # shape (batch_size,)\n",
    "        \n",
    "        return (classifications == 1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5104ea-1327-4fbd-bd6b-edcbec967bd4",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd05912ce3053e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Models fine-tuning on MultiCW dataset\n",
    "- Fine-tuning of xlm-RoBERTa, mDeBERTa anb LESA models on MultiCW train set\n",
    "- Evaluation on MultiCW test set\n",
    "\n",
    "#### XLM-RoBERTa model:\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support   |\n",
    "| ---------------- | --------- | ------ | -------- | --------- |\n",
    "| 0                | 0.87      | 0.88   | 0.87     | 9269      |\n",
    "| 1                | 0.88      | 0.87   | 0.87     | 9175      |\n",
    "| **Accuracy**     |           |        | **0.87** | **18444** |\n",
    "| **Macro avg**    | 0.87      | 0.87   | 0.87     | 18444     |\n",
    "| **Weighted avg** | 0.87      | 0.87   | 0.87     | 18444     |\n",
    "                    MultiCW Overall.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.81      | 0.82   | 0.81     | 4744     |\n",
    "| 1                | 0.81      | 0.80   | 0.81     | 4650     |\n",
    "| **Accuracy**     |           |        | **0.81** | **9394** |\n",
    "| **Macro avg**    | 0.81      | 0.81   | 0.81     | 9394     |\n",
    "| **Weighted avg** | 0.81      | 0.81   | 0.81     | 9394     |\n",
    "                MultiCW Noisy Part.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.93      | 0.94   | 0.94     | 4525     |\n",
    "| 1                | 0.94      | 0.93   | 0.93     | 4525     |\n",
    "| **Accuracy**     |           |        | **0.93** | **9050** |\n",
    "| **Macro avg**    | 0.93      | 0.93   | 0.93     | 9050     |\n",
    "| **Weighted avg** | 0.93      | 0.93   | 0.93     | 9050     |\n",
    "                MultiCW Structured Part.\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### mDeBERTa model:\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support   |\n",
    "| ---------------- | --------- | ------ | -------- | --------- |\n",
    "| 0                | 0.89      | 0.86   | 0.87     | 9269      |\n",
    "| 1                | 0.86      | 0.89   | 0.88     | 9175      |\n",
    "| **Accuracy**     |           |        | **0.88** | **18444** |\n",
    "| **Macro avg**    | 0.88      | 0.88   | 0.88     | 18444     |\n",
    "| **Weighted avg** | 0.88      | 0.88   | 0.88     | 18444     |\n",
    "                    MultiCW Overall.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.84      | 0.80   | 0.82     | 4744     |\n",
    "| 1                | 0.81      | 0.84   | 0.82     | 4650     |\n",
    "| **Accuracy**     |           |        | **0.82** | **9394** |\n",
    "| **Macro avg**    | 0.82      | 0.82   | 0.82     | 9394     |\n",
    "| **Weighted avg** | 0.82      | 0.82   | 0.82     | 9394     |\n",
    "                MultiCW Noisy Part.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.95      | 0.91   | 0.93     | 4525     |\n",
    "| 1                | 0.92      | 0.95   | 0.93     | 4525     |\n",
    "| **Accuracy**     |           |        | **0.93** | **9050** |\n",
    "| **Macro avg**    | 0.93      | 0.93   | 0.93     | 9050     |\n",
    "| **Weighted avg** | 0.93      | 0.93   | 0.93     | 9050     |\n",
    "                MultiCW Structured Part.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67e86a4-96fa-4fc9-bc1f-5d7ee253bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;42mXLM-RoBERTa model:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 07:29:50.732261: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.787211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.791843: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.796114: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.800168: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.802899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.937409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.939480: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.941555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-01 07:29:50.943717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4134 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-09-01 07:29:53.127788: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2025-09-01 07:29:55.067900: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 401 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1;30;42mMultiCW overall:\u001b[0m\n",
      "Running classification:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756704598.034464 1441906 service.cc:145] XLA service 0x7fc4b002cdc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756704598.034524 1441906 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-09-01 07:29:58.117842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-01 07:29:58.332846: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756704599.611815 1441982 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2670', 348 bytes spill stores, 356 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/577\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34:16\u001b[0m 4s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756704600.414404 1441906 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m576/577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 195ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756704714.817255 1442071 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m577/577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 200ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      9269\n",
      "           1       0.88      0.87      0.87      9175\n",
      "\n",
      "    accuracy                           0.87     18444\n",
      "   macro avg       0.87      0.87      0.87     18444\n",
      "weighted avg       0.87      0.87      0.87     18444\n",
      "\n",
      "Running classification:\n",
      "\u001b[1m293/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 198ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756704776.305381 1442191 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "I0000 00:00:1756704776.697019 1442190 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 372 bytes spill stores, 380 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 206ms/step\n",
      "Done.\n",
      "\u001b[1;30;43mMultiCW Noisy Part:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81      4744\n",
      "           1       0.81      0.80      0.81      4650\n",
      "\n",
      "    accuracy                           0.81      9394\n",
      "   macro avg       0.81      0.81      0.81      9394\n",
      "weighted avg       0.81      0.81      0.81      9394\n",
      "\n",
      "Running classification:\n",
      "\u001b[1m282/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 198ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756704835.888722 1442280 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "I0000 00:00:1756704836.010144 1442286 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 352 bytes spill stores, 356 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 206ms/step\n",
      "Done.\n",
      "\u001b[1;30;43mMultiCW Structured Part:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      4525\n",
      "           1       0.94      0.93      0.93      4525\n",
      "\n",
      "    accuracy                           0.93      9050\n",
      "   macro avg       0.93      0.93      0.93      9050\n",
      "weighted avg       0.93      0.93      0.93      9050\n",
      "\n",
      "\u001b[1;30;42mmDBERTa model:\u001b[0m\n",
      "Loading full model from: Models/mdb-multicw-2e6-5e.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "  instance.compile_from_config(compile_config)\n",
      "2025-09-01 07:34:13.255647: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 771072000 exceeds 10% of free system memory.\n",
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 406 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1;30;42mMultiCW overall:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m577/577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 339ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87      9269\n",
      "           1       0.86      0.89      0.88      9175\n",
      "\n",
      "    accuracy                           0.88     18444\n",
      "   macro avg       0.88      0.88      0.88     18444\n",
      "weighted avg       0.88      0.88      0.88     18444\n",
      "\n",
      "Running classification:\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 341ms/step\n",
      "Done.\n",
      "\u001b[1;30;43mMultiCW Noisy Part:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      4744\n",
      "           1       0.81      0.84      0.82      4650\n",
      "\n",
      "    accuracy                           0.82      9394\n",
      "   macro avg       0.82      0.82      0.82      9394\n",
      "weighted avg       0.82      0.82      0.82      9394\n",
      "\n",
      "Running classification:\n",
      "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 343ms/step\n",
      "Done.\n",
      "\u001b[1;30;43mMultiCW Structured Part:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      4525\n",
      "           1       0.92      0.95      0.93      4525\n",
      "\n",
      "    accuracy                           0.93      9050\n",
      "   macro avg       0.93      0.93      0.93      9050\n",
      "weighted avg       0.93      0.93      0.93      9050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "detector = None\n",
    "models = ['xlm', 'mdb']\n",
    "\n",
    "for model in models:\n",
    "    if model == 'xlm':\n",
    "        print(f'{h_green}XLM-RoBERTa model:{h_stop}')\n",
    "        detector = XLMRobertaModel()\n",
    "    if model == 'mdb':\n",
    "        print(f'{h_green}mDBERTa model:{h_stop}')\n",
    "        detector = MDeBertaModel()\n",
    "\n",
    "    if not detector.load_model(model_name=f'{model}-multicw-2e6-5e'):\n",
    "        print(f'{h_yellow}No model found. Initiating fine-tuning:{h_stop}')\n",
    "        # Note: Works well with a small learning rate (e.g. 3e-6)\n",
    "        detector.train_model(multicw_train, multicw_dev, epochs=5, learn_rate=2e-6, lang='en', model_name=f'{model}-multicw')\n",
    "    \n",
    "    print(f'{h_yellow}MultiCW overall:{h_stop}')\n",
    "    _, report = detector.detect_claims(multicw_test)\n",
    "    print(report)\n",
    "    \n",
    "    test_noisy = multicw_test.loc[multicw_test['style']=='noisy']\n",
    "    _, report = detector.detect_claims(test_noisy, verbose=False)\n",
    "    print(f'{h_yellow}MultiCW Noisy Part:{h_stop}')\n",
    "    print(report)\n",
    "    \n",
    "    test_strut = multicw_test.loc[multicw_test['style']=='struct']\n",
    "    _, report = detector.detect_claims(test_strut, verbose=False)\n",
    "    print(f'{h_yellow}MultiCW Structured Part:{h_stop}')\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43847a8ba1952a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Out-domain evaluation\n",
    "- Evaluation of the fine-tuned models on 4 more languages obtained from source datasets.\n",
    "\n",
    "#### XLM-RoBERTa model:\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support   |\n",
    "| ---------------- | --------- | ------ | -------- | --------- |\n",
    "| 0                | 0.94      | 0.75   | 0.84     | 16000     |\n",
    "| 1                | 0.76      | 0.95   | 0.85     | 13647     |\n",
    "| **Accuracy**     |           |        | **0.84** | **29647** |\n",
    "| **Macro avg**    | 0.85      | 0.85   | 0.84     | 29647     |\n",
    "| **Weighted avg** | 0.86      | 0.84   | 0.84     | 29647     |\n",
    "                    MultiCW Overall.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.91      | 0.70   | 0.79     | 8000     |\n",
    "| 1                | 0.76      | 0.93   | 0.84     | 8000     |\n",
    "| **Accuracy**     |           |        | **0.82** | **16000** |\n",
    "| **Macro avg**    | 0.83      | 0.82   | 0.81     | 16000     |\n",
    "| **Weighted avg** | 0.83      | 0.82   | 0.81     | 16000     |\n",
    "                MultiCW Noisy Part.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.97      | 0.80   | 0.88     | 8000     |\n",
    "| 1                | 0.78      | 0.97   | 0.86     | 5647     |\n",
    "| **Accuracy**     |           |        | **0.87** | **13647** |\n",
    "| **Macro avg**    | 0.87      | 0.88   | 0.87     | 13647     |\n",
    "| **Weighted avg** | 0.89      | 0.87   | 0.87     | 13647     |\n",
    "                MultiCW Structured Part.\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### mDBERTa model:\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support   |\n",
    "| ---------------- | --------- | ------ | -------- | --------- |\n",
    "| 0                | 0.97      | 0.74   | 0.84     | 16000     |\n",
    "| 1                | 0.76      | 0.97   | 0.85     | 13647     |\n",
    "| **Accuracy**     |           |        | **0.85** | **29647** |\n",
    "| **Macro avg**    | 0.86      | 0.86   | 0.85     | 29647     |\n",
    "| **Weighted avg** | 0.87      | 0.85   | 0.85     | 29647     |\n",
    "                    MultiCW Overall.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.95      | 0.68   | 0.80     | 8000     |\n",
    "| 1                | 0.75      | 0.96   | 0.85     | 8000     |\n",
    "| **Accuracy**     |           |        | **0.82** | **16000** |\n",
    "| **Macro avg**    | 0.85      | 0.82   | 0.82     | 16000     |\n",
    "| **Weighted avg** | 0.85      | 0.82   | 0.82     | 16000     |\n",
    "                MultiCW Noisy Part.\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Class            | Precision | Recall | F1-score | Support  |\n",
    "| ---------------- | --------- | ------ | -------- | -------- |\n",
    "| 0                | 0.98      | 0.79   | 0.88     | 8000     |\n",
    "| 1                | 0.77      | 0.98   | 0.86     | 5647     |\n",
    "| **Accuracy**     |           |        | **0.87** | **13647** |\n",
    "| **Macro avg**    | 0.88      | 0.89   | 0.87     | 13647     |\n",
    "| **Weighted avg** | 0.90      | 0.87   | 0.87     | 13647     |\n",
    "                MultiCW Structured Part.\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e6f104-4696-4390-9d44-b9e40b88c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;42mXLM-RoBERTa model:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:50:19.039339: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.171982: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.175233: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.179386: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.182559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.185120: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.327346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.329724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.331727: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-09 15:50:19.333744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4174 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-09-09 15:50:21.641614: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2025-09-09 15:50:23.731627: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 401 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1;30;42mMultiCW overall:\u001b[0m\n",
      "Running classification:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757425826.950727    4114 service.cc:145] XLA service 0x7f0c38009bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1757425826.950786    4114 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-09-09 15:50:27.030880: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-09 15:50:27.253428: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757425828.487016    4229 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2670', 348 bytes spill stores, 356 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/927\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58:27\u001b[0m 4s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757425829.360802    4114 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m926/927\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 195ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757426012.369490    4420 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2688', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m927/927\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 199ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.75      0.84     16000\n",
      "           1       0.76      0.95      0.85     13647\n",
      "\n",
      "    accuracy                           0.84     29647\n",
      "   macro avg       0.85      0.85      0.84     29647\n",
      "weighted avg       0.86      0.84      0.84     29647\n",
      "\n",
      "\u001b[1;30;43mMultiCW Noisy Part:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 198ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79      8000\n",
      "           1       0.76      0.93      0.84      8000\n",
      "\n",
      "    accuracy                           0.82     16000\n",
      "   macro avg       0.83      0.82      0.81     16000\n",
      "weighted avg       0.83      0.82      0.81     16000\n",
      "\n",
      "\u001b[1;30;43mMultiCW Structured Part:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 199ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88      8000\n",
      "           1       0.78      0.97      0.86      5647\n",
      "\n",
      "    accuracy                           0.87     13647\n",
      "   macro avg       0.87      0.88      0.87     13647\n",
      "weighted avg       0.89      0.87      0.87     13647\n",
      "\n",
      "\u001b[1;30;42mmDBERTa model:\u001b[0m\n",
      "Loading full model from: Models/mdb-multicw-2e6-5e.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "  instance.compile_from_config(compile_config)\n",
      "2025-09-09 15:56:55.885372: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 771072000 exceeds 10% of free system memory.\n",
      "/home/hyben/.conda/envs/MultiCW-finetune/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 406 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1;30;42mMultiCW overall:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m927/927\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 337ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.74      0.84     16000\n",
      "           1       0.76      0.97      0.85     13647\n",
      "\n",
      "    accuracy                           0.85     29647\n",
      "   macro avg       0.86      0.86      0.85     29647\n",
      "weighted avg       0.87      0.85      0.85     29647\n",
      "\n",
      "\u001b[1;30;43mMultiCW Noisy Part:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 335ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.80      8000\n",
      "           1       0.75      0.96      0.85      8000\n",
      "\n",
      "    accuracy                           0.82     16000\n",
      "   macro avg       0.85      0.82      0.82     16000\n",
      "weighted avg       0.85      0.82      0.82     16000\n",
      "\n",
      "\u001b[1;30;43mMultiCW Structured Part:\u001b[0m\n",
      "Running classification:\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 336ms/step\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88      8000\n",
      "           1       0.77      0.98      0.86      5647\n",
      "\n",
      "    accuracy                           0.87     13647\n",
      "   macro avg       0.88      0.89      0.87     13647\n",
      "weighted avg       0.90      0.87      0.87     13647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "detector = None\n",
    "models = ['xlm', 'mdb']\n",
    "\n",
    "for model in models:\n",
    "    if model == 'xlm':\n",
    "        print(f'{h_green}XLM-RoBERTa model:{h_stop}')\n",
    "        detector = XLMRobertaModel()\n",
    "    if model == 'mdb':\n",
    "        print(f'{h_green}mDBERTa model:{h_stop}')\n",
    "        detector = MDeBertaModel()\n",
    "\n",
    "    if not detector.load_model(model_name=f'{model}-multicw-2e6-5e'):\n",
    "        print(f'{h_yellow}No model found. Initiating fine-tuning:{h_stop}')\n",
    "        # Note: Works well with a small learning rate (e.g. 3e-6)\n",
    "        detector.train_model(multicw_train, multicw_dev, epochs=5, learn_rate=2e-6, lang='en', model_name=f'{model}-multicw')\n",
    "    \n",
    "    print(f'{h_yellow}MultiCW overall:{h_stop}')\n",
    "    _, report = detector.detect_claims(multicw_ood)\n",
    "    print(report)\n",
    "    \n",
    "    test_noisy = multicw_ood.loc[multicw_ood['style']=='noisy']\n",
    "    print(f'{h_yellow}MultiCW Noisy Part:{h_stop}')\n",
    "    _, report = detector.detect_claims(test_noisy, verbose=False)\n",
    "    print(report)\n",
    "    \n",
    "    test_strut = multicw_ood.loc[multicw_ood['style']=='struc']\n",
    "    print(f'{h_yellow}MultiCW Structured Part:{h_stop}')\n",
    "    _, report = detector.detect_claims(test_strut, verbose=False)\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 2834,
     "modelInstanceId": 4721,
     "sourceId": 6189,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 2820,
     "modelInstanceId": 4688,
     "sourceId": 6067,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MultiCW-finetune",
   "language": "python",
   "name": "multicw-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0abbc91f30b04bd3b464ab626caa4ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2511f785d020410e851c8f198278c7da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "cpu",
       "gpu"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Storage mode:",
      "description_tooltip": null,
      "disabled": false,
      "index": 1,
      "layout": "IPY_MODEL_8af9da3904434c8db72bf48038475c9e",
      "style": "IPY_MODEL_0abbc91f30b04bd3b464ab626caa4ef6"
     }
    },
    "2c2931510812437899b50874520958b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353fd4c3ba06412d9431ead32ef52b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Batch size:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_8642bd2f2a174d2eafd1ae06aaa494e5",
      "max": 32,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_6305e85181ee4afa84499a116c911b1c",
      "value": 8
     }
    },
    "5b36326a73a84c80aadbe97684b4083c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfac425cbea74dbca85bc28aec8fca84",
       "IPY_MODEL_353fd4c3ba06412d9431ead32ef52b9c",
       "IPY_MODEL_e2a27094e6ab478e887d6dbd0d166255",
       "IPY_MODEL_c498aabcc4754ea69cc7939e23b295f2",
       "IPY_MODEL_f0a443c22c71401cadcd0ed9d720a50b",
       "IPY_MODEL_2511f785d020410e851c8f198278c7da"
      ],
      "layout": "IPY_MODEL_f151a00a84a444a2ac3dde95d6449f76"
     }
    },
    "6305e85181ee4afa84499a116c911b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "6d2245fa0357414793ec484e42a407ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "702fc0ae0e9646e195974358ada8698e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "8642bd2f2a174d2eafd1ae06aaa494e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8988b2e3116348faae276ec36bf6b2af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8af9da3904434c8db72bf48038475c9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfac425cbea74dbca85bc28aec8fca84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "mDeBERTa",
       "XLM-RoBERTa",
       "LESA"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Model:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_ecacd507e05b492dbdcfd924c6c7e039",
      "style": "IPY_MODEL_8988b2e3116348faae276ec36bf6b2af"
     }
    },
    "c1fa2f69f0a147e891184b3d41decae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "c498aabcc4754ea69cc7939e23b295f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatLogSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatLogSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatLogSliderView",
      "base": 10,
      "continuous_update": true,
      "description": "Learning Rate:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6d2245fa0357414793ec484e42a407ce",
      "max": -1,
      "min": -10,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".1e",
      "step": 0.1,
      "style": "IPY_MODEL_cf376d4917444367af900a2ea6fbfbd1",
      "value": 3e-05
     }
    },
    "cf376d4917444367af900a2ea6fbfbd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial",
      "handle_color": null
     }
    },
    "e2a27094e6ab478e887d6dbd0d166255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "Batch chunk size:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_fb777cd857e848febd846dabb6f351e4",
      "max": 32,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_c1fa2f69f0a147e891184b3d41decae5",
      "value": 8
     }
    },
    "ecacd507e05b492dbdcfd924c6c7e039": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0a443c22c71401cadcd0ed9d720a50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "Classifier threshold:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2c2931510812437899b50874520958b0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 1e-05,
      "style": "IPY_MODEL_702fc0ae0e9646e195974358ada8698e",
      "value": 0.5
     }
    },
    "f151a00a84a444a2ac3dde95d6449f76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb777cd857e848febd846dabb6f351e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
